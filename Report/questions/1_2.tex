Nous allons maintenant nous attarder sur l'aspect théorique de l'algorithme de 
Metropolis-Hastings avant de passer à l'aspect pratique.

\subsubsection{}
On veut montrer que $\pi_0$ est une distribution stationnaire sachant que une matrice de transition $Q$ et une distribution initiale $\pi_0$ 
d'une chaîne de Markov invariante dans le temps qui satisfont les équations de balance détaillée :
\begin{equation*}
  \forall i,j \in \{ 1,\dots,N \}, \pi_0(i)[Q]_{i,j} = \pi_0(j)[Q]_{j,i}
\end{equation*}

On reprend donc les définitions de $\pi_0$ et de $Q$ :
\begin{equation*}
  \pi_0 = [P(X_0=x_1) \dots P(X_0=x_k) \dots P(X_0=x_n)]
\end{equation*}
$$Q = \begin{pmatrix}
  P(X_1 = x_1|X_0=x_1) & \dots & P(X_1 = x_n|X_0=x_1)\\
  \dots & P(X_1 = x_n|X_0=x_n) & \dots\\
  P(X_1 = x_1|X_0=x_n) & \dots & P(X_1 = x_n|X_0=x_n)
\end{pmatrix}$$

Sachant que la chaîne de Markov est invariante, on a $Q_0 = Q_1 = \dots = Q_n = Q$ et on sait que la matrice de transition et la distribution initiale respectent les équations de balance détaillée :

\begin{align*}
  \forall i,j \in \{ 1,\dots,N \}, \pi_0(i)[Q]_{i,j} &= \pi_0(j)[Q]_{j,i}\\
  P(X_0=x_i)P(X_1=x_j|X_0=x_i) &= P(X_0 = x_j)P(X_1=x_i|X_0=X_j)\\
  P(X_0=x_i)\frac{P(X_1=x_j,X_0=x_i)}{P(X_0=x_i)} &= P(X_0=x_j)\frac{P(X_1=x_i,X_0=x_j)}{P(X_0=x_j)} \text{ par Bayes}\\
  P(X_1=x_j,X_0=x_i) &= P(X_1=x_i,X_0=x_j) \forall i,j
\end{align*}

On sait également que au vu de la forme de $\pi_0$ et $Q$ vue plus haut et que $\pi_1 = \pi_0 * Q$ :

\begin{align*}
  P(X_1=x_j) &= \sum_{k=1}^n P(X_0=x_k)P(X_1=x_j|X_0=x_k)\\
  P(X_1=x_j) &= \sum_{k=1}^n P(X_0=x_k)\frac{P(X_1=x_j,X_0=x_k)}{P(X_0=x_k)}\\
  P(X_1=x_j) &= \sum_{k=1}^n P(X_1=x_j,X_0=x_k) = \sum_{k=1}^n P(X_1=x_k,X_0=x_j)\\
  P(X_1=x_j) &= \sum_{k=1}^n P(X_1=x_k,X_0=x_j) = P(X_0=x_j)
\end{align*}

On vérifie cela pour chaque $j$, ce qui montre que $\pi_0 = \pi_1$ et que $\pi_0$ est une distribution stationnaire. 

\subsubsection{}


